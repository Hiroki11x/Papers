# Papers
Survey

## Lists

### @Motokawa
##### Hessisan
- Sharpness-Aware Minimization for Efficiently Improving Generalization
https://github.com/MLHPC/Papers/issues/51
- Modular Block-diagonal Curvature Approximations for Feedforward Architectures
https://github.com/MLHPC/Papers/issues/31
- When Does Preconditioning Help or Hurt Generalization?
https://github.com/MLHPC/Papers/issues/26
- Assessing Local Generalization Capability in Deep Models 
https://github.com/MLHPC/Papers/issues/28

##### Sensitivity and Variance
- Sensitivity and Generalization in Neural Networks: an Empirical Study
https://github.com/MLHPC/Papers/issues/11
- Understanding Why Neural Networks Generalize Well Through GSNR of Parameters
https://github.com/MLHPC/Papers/issues/4

##### KFAC
- WoodFisher: Efficient Second-Order Approximation for Neural Network Compression
https://github.com/MLHPC/Papers/issues/64

##### SGD Noise
- An Empirical Study of Large-Batch Stochastic Gradient Descent with Structured Covariance Noise
https://github.com/MLHPC/Papers/issues/2
- The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning (Escaping Efficiency)
https://github.com/MLHPC/Papers/issues/3
- The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects
https://github.com/MLHPC/Papers/issues/7

--------------------------------------------------------------------------------------------------------------

### @Takeuchi
##### DNN Training Dynamics
- How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective
https://github.com/MLHPC/Papers/issues/22
- On the Geometry of Generalization and Memorization in Deep Neural Networks
https://github.com/MLHPC/Papers/issues/62

##### Optimization, Regularization
- On regularization of gradient descent, layer imbalance and flat minima
https://github.com/MLHPC/Papers/issues/69
- On the Variance of the Adaptive Learning Rate and Beyond
https://github.com/MLHPC/Papers/issues/73
- On the diffusion approximation of nonconvex stochastic gradient descent
https://github.com/MLHPC/Papers/issues/23

--------------------------------------------------------------------------------------------------------------

### @Takagi
- Rethinking Parameter Counting in Deep Models: Effective Dimensionality Revisited
https://github.com/MLHPC/Papers/issues/54
- Understanding self-supervised learning with dual deep networks
https://github.com/MLHPC/Papers/issues/60

--------------------------------------------------------------------------------------------------------------

### @Naganuma

##### Relationship wrt DataPoint
- Small Data, Big Decisions: Model Selection in the Small-Data Regime
https://github.com/MLHPC/Papers/issues/72

##### Generalization Measures
- Fantastic Generalization Measures and Where to Find Them
https://github.com/MLHPC/Papers/issues/12
- On the interplay between noise and curvature and its effect on optimization and generalization
https://github.com/MLHPC/Papers/issues/6
- Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization
https://github.com/MLHPC/Papers/issues/71

##### Large Batch Training
- Measuring the Effects of Data Parallelism on Neural Network Training
https://github.com/MLHPC/Papers/issues/16
- Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model
https://github.com/MLHPC/Papers/issues/10
- Don't Use Large Mini-batches, Use Local SGD
https://github.com/MLHPC/Papers/issues/9
- An Empirical Model of Large-Batch Training
https://github.com/MLHPC/Papers/issues/8

##### Saddle Point
- Gradient Descent Can Take Exponential Time to Escape Saddle Points
https://github.com/MLHPC/Papers/issues/14
- How to Escape Saddle Points Efficiently
https://github.com/MLHPC/Papers/issues/18

##### Variance of Gradient and Gradient Noise
- Gradient Diversity: a Key Ingredient for Scalable Distributed Learning
https://github.com/MLHPC/Papers/issues/13
- On the Generalization Benefit of Noise in Stochastic Gradient Descent
https://github.com/MLHPC/Papers/issues/42

##### Dynamics
- Direction Matters: On the Implicit Regularization Effect of Stochastic Gradient Descent with Moderate Learning Rate 
https://github.com/MLHPC/Papers/issues/58

##### OOD
- SWAD: Domain Generalization by Seeking Flat Minima
https://github.com/MLHPC/Papers/issues/92

##### Generalizaton
- How Does Mixup Help With Robustness and Generalization?
https://github.com/MLHPC/Papers/issues/137

--------------------------------------------------------------------------------------------------------------

### @Fujimori
- Bad Global Minima Exist and SGD Can Reach Them
https://github.com/MLHPC/Papers/issues/67

--------------------------------------------------------------------------------------------------------------

### @Ishida
- Large Batch Optimization for Deep Learning Using New Complete Layer-Wise Adaptive Rate Scaling
https://github.com/MLHPC/Papers/issues/81

- Spectral Normalization for Generative Adversarial Networks 
https://github.com/MLHPC/Papers/issues/143
